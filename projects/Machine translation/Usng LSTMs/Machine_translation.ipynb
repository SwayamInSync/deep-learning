{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "g0O5Zm7122fA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "t7jc0Tpp3CS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, frequency_threshold):\n",
        "        self.itos = {\n",
        "            0: '<PAD>',\n",
        "            1: '<SOS>',\n",
        "            2: '<EOS>',\n",
        "            3: '<UNK>'\n",
        "        }\n",
        "\n",
        "        self.stoi = {\n",
        "            '<PAD>': 0,\n",
        "            '<SOS>': 1,\n",
        "            '<EOS>': 2,\n",
        "            '<UNK>': 3\n",
        "        }\n",
        "\n",
        "        self.frequency_threshold = frequency_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def __getitem__(self, value):\n",
        "      if isinstance(value, int):\n",
        "        return self.itos[value]\n",
        "      else:\n",
        "        return self.stoi[value]\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return []\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "                if frequencies[word] == self.frequency_threshold:\n",
        "                    self.itos[idx] = word\n",
        "                    self.stoi[word] = idx\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi['<UNK>']\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "    def un_numericalize(self, encoding):\n",
        "        return \" \".join([\n",
        "            self.itos[token.data.item()] if token.data.item() in self.itos else self.itos[3]\n",
        "            for token in encoding\n",
        "        ])\n",
        "\n",
        "\n",
        "class EngVocabulary(Vocabulary):\n",
        "    def __init__(self, frequency_threshold):\n",
        "        super().__init__(frequency_threshold)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "class FrVocabulary(Vocabulary):\n",
        "    def __init__(self, frequency_threshold):\n",
        "        super().__init__(frequency_threshold)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]"
      ],
      "metadata": {
        "id": "RMMwX_S63HbZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, frequency_threshold_en=2, frequency_threshold_fr=1, vocab=None):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.english = open(os.path.join(root_dir, \"english.txt\")).read().split(\"\\n\")[:-1]\n",
        "        self.french = open(os.path.join(root_dir, \"french.txt\")).read().split(\"\\n\")[:-1]\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab_en = EngVocabulary(frequency_threshold_en)\n",
        "            self.vocab_fr = FrVocabulary(frequency_threshold_fr)\n",
        "            self.vocab_en.build_vocabulary(self.english)\n",
        "            self.vocab_fr.build_vocabulary(self.french)\n",
        "        else:\n",
        "            self.vocab_en = vocab[0]\n",
        "            self.vocab_fr = vocab[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        english_sentence = self.english[index]\n",
        "        french_sentence = self.french[index]\n",
        "        numericalized_en = [self.vocab_en.stoi['<SOS>']]\n",
        "        numericalized_en += self.vocab_en.numericalize(english_sentence)\n",
        "        numericalized_en.append(self.vocab_en.stoi['<EOS>'])\n",
        "        numericalized_en = torch.tensor(numericalized_en)\n",
        "\n",
        "        numericalized_fr = [self.vocab_fr.stoi['<SOS>']]\n",
        "        numericalized_fr += self.vocab_fr.numericalize(french_sentence)\n",
        "        numericalized_fr.append(self.vocab_fr.stoi['<EOS>'])\n",
        "        numericalized_fr = torch.tensor(numericalized_fr)\n",
        "\n",
        "        return numericalized_fr, numericalized_en\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx_fr, pad_idx_en):\n",
        "        self.pad_idx_fr = pad_idx_fr\n",
        "        self.pad_idx_en = pad_idx_en\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        fr = [item[0] for item in batch]\n",
        "        lengths = torch.tensor([item.shape[0] for item in fr])\n",
        "        en = [item[1] for item in batch]\n",
        "        fr = pad_sequence(fr, padding_value=self.pad_idx_fr)\n",
        "        en = pad_sequence(en, padding_value=self.pad_idx_en)\n",
        "        return (fr, lengths), en\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size, shuffle, vocab=None):\n",
        "    dataset = CustomDataset(root_dir, vocab=vocab)\n",
        "    pad_idx_en = dataset.vocab_en.stoi['<PAD>']\n",
        "    pad_idx_fr = dataset.vocab_fr.stoi['<PAD>']\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size, shuffle=shuffle,\n",
        "        collate_fn=MyCollate(pad_idx_fr, pad_idx_en),\n",
        "    )\n",
        "    return dataset, loader"
      ],
      "metadata": {
        "id": "F3RMY0YlBCHC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/019/853/original/data.zip\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TitP6Vb5BVGZ",
        "outputId": "56d05f8c-a5fd-4b32-df51-ff464a449c63"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-16 10:51:32--  https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/019/853/original/data.zip\n",
            "Resolving d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)... 13.35.18.148, 13.35.18.37, 13.35.18.5, ...\n",
            "Connecting to d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)|13.35.18.148|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1269302 (1.2M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "\rdata.zip              0%[                    ]       0  --.-KB/s               \rdata.zip            100%[===================>]   1.21M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-11-16 10:51:32 (101 MB/s) - ‘data.zip’ saved [1269302/1269302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, bidirectional=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, source):\n",
        "        # print(\"input shape: \", x.shape, self.embedding)\n",
        "        x, lengths = source\n",
        "        embedding = self.embedding(x)\n",
        "        packed_embeds = pack_padded_sequence(embedding, lengths.to('cpu'), enforce_sorted=False)\n",
        "        # print(\"embedding shape: \", embedding.shape)\n",
        "        packed_output, (hidden, cell) = self.lstm(embedding)\n",
        "        # print(\"output shape: \", output.shape, hidden.shape, cell.shape)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # since x is only one word so need to add extra dimension of 1\n",
        "        # x.shape: (1, batch_size)\n",
        "        x = x.unsqueeze(0)\n",
        "        embedding = self.embedding(x)\n",
        "        # embedding.shape: (1, batch_size, 300)\n",
        "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "        predictions = self.fc(output)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder,  decoder, eng_vocab_size, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.eng_vocab_size = eng_vocab_size\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source[0].shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        # picking 0s because in vocab 0 stands for <PAD> to pad the remaining length\n",
        "        outputs = torch.zeros(\n",
        "            (target_len, batch_size, self.eng_vocab_size)).to(self.device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # passing the first character for each sencetence in batch\n",
        "        x = target[0] # <SOS> shape: (1, B, embed_size)\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            best_guess = output.argmax(1)  # returns index of maximum value\n",
        "            x = target[t] if random.random(\n",
        "            ) < teacher_force_ratio else best_guess\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, source, max_len=100):\n",
        "        result = []\n",
        "        result.append(1)  # index of <SOS> token\n",
        "        hidden, cell = self.encoder(source)\n",
        "        hidden, cell = hidden.unsqueeze(1), cell.unsqueeze(1)\n",
        "        x = torch.tensor([1]).to(self.device)\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            best_guess = output.argmax()\n",
        "            result.append(best_guess)\n",
        "            print(best_guess.data.item())\n",
        "            if best_guess.data.item() == 2:\n",
        "                return torch.tensor(result).to(self.device)\n",
        "        result.append(2)  # index of <EOS> token\n",
        "\n",
        "        return torch.tensor(result).to(self.device)"
      ],
      "metadata": {
        "id": "ctIQwMfABUlF"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, train_loader = get_loader(\"data/train\", batch_size=128, shuffle=True)\n",
        "val_set, val_loader = get_loader(\"data/val\", batch_size=128, shuffle=True,\n",
        "                                  vocab=[train_set.vocab_en, train_set.vocab_fr])"
      ],
      "metadata": {
        "id": "FePoknMzL893"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input_size_encoder = len(train_set.vocab_fr)\n",
        "input_size_decoder = len(train_set.vocab_en)\n",
        "output_size = len(train_set.vocab_en)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 4\n",
        "num_layers_decoder = 4\n",
        "bidirectional = True\n",
        "if bidirectional:\n",
        "  num_layers_decoder = 2*num_layers\n",
        "\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "pad_idx = train_set.vocab_en.stoi[\"<PAD>\"]"
      ],
      "metadata": {
        "id": "X7wlMJrdMJaB"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, encoder_dropout, bidirectional).to(device)\n",
        "decoder_net = Decoder(input_size_decoder, decoder_embedding_size,\n",
        "                      hidden_size, output_size, num_layers_decoder, decoder_dropout).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net, len(train_set.vocab_en), device).to(device)"
      ],
      "metadata": {
        "id": "EpVoFACmL4ZW"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "iBPcCWe-SxnB"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_val_loss = 0\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for batch_idx, ((french, lengths), english) in train_loop:\n",
        "        french = french.to(device)\n",
        "        english = english.to(device)\n",
        "        output = model((french, lengths), english)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        english = english[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, english)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.data.item()\n",
        "        train_loop.set_description(f\"Epoch: {epoch}/{num_epochs}\")\n",
        "        train_loop.set_postfix({\"batch_loss\": loss.data.item(), \"train_loss\":train_loss, \"val_loss\": total_val_loss})\n",
        "        if epoch % 100 == 0:\n",
        "          torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "          print(\"model saving done\")\n",
        "        # ---- validation-----\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        val_loss = 0\n",
        "        val_loop = tqdm(val_loader, total=len(val_loader), leave=False)\n",
        "        for (french, lengths), english in val_loop:\n",
        "            french = french.to(device)\n",
        "            english = english.to(device)\n",
        "            output = model((french, lengths), english)\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            english = english[1:].reshape(-1)\n",
        "            loss = criterion(output, english).data.item()\n",
        "            val_loss += loss\n",
        "\n",
        "            val_loop.set_description(\"Validating\")\n",
        "            val_loop.set_postfix({'val loss': loss})\n",
        "        total_val_loss = val_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5n4zKKdSpR-",
        "outputId": "b6ecf801-cb74-426a-d391-49f5772e818d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 21/1000:  33%|███▎      | 76/227 [01:53<04:10,  1.66s/it, batch_loss=1.2, train_loss=84.7, val_loss=31.2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6LJO-S3UWaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}