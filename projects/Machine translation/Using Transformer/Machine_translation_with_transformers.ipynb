{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fUb0LnfE_QI3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyobEEj3_fWb",
        "outputId": "e385691d-63a4-401e-f9e7-764846cfce64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 84.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, frequency_threshold):\n",
        "        self.itos = {\n",
        "            0: '<PAD>',\n",
        "            1: '<SOS>',\n",
        "            2: '<EOS>',\n",
        "            3: '<UNK>'\n",
        "        }\n",
        "\n",
        "        self.stoi = {\n",
        "            '<PAD>': 0,\n",
        "            '<SOS>': 1,\n",
        "            '<EOS>': 2,\n",
        "            '<UNK>': 3\n",
        "        }\n",
        "\n",
        "        self.frequency_threshold = frequency_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def __getitem__(self, value):\n",
        "      if isinstance(value, int):\n",
        "        return self.itos[value]\n",
        "      else:\n",
        "        return self.stoi[value]\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return []\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "                if frequencies[word] == self.frequency_threshold:\n",
        "                    self.itos[idx] = word\n",
        "                    self.stoi[word] = idx\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi['<UNK>']\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "    def un_numericalize(self, encoding):\n",
        "        return \" \".join([\n",
        "            self.itos[token.data.item()] if token.data.item() in self.itos else self.itos[3]\n",
        "            for token in encoding\n",
        "        ])\n",
        "\n",
        "\n",
        "class EngVocabulary(Vocabulary):\n",
        "    def __init__(self, frequency_threshold):\n",
        "        super().__init__(frequency_threshold)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "class FrVocabulary(Vocabulary):\n",
        "    def __init__(self, frequency_threshold):\n",
        "        super().__init__(frequency_threshold)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]"
      ],
      "metadata": {
        "id": "cVyIZSyu_g7h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, frequency_threshold_en=2, frequency_threshold_fr=1, vocab=None):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.english = open(os.path.join(root_dir, \"english.txt\")).read().split(\"\\n\")[:-1]\n",
        "        self.french = open(os.path.join(root_dir, \"french.txt\")).read().split(\"\\n\")[:-1]\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab_en = EngVocabulary(frequency_threshold_en)\n",
        "            self.vocab_fr = FrVocabulary(frequency_threshold_fr)\n",
        "            self.vocab_en.build_vocabulary(self.english)\n",
        "            self.vocab_fr.build_vocabulary(self.french)\n",
        "        else:\n",
        "            self.vocab_en = vocab[0]\n",
        "            self.vocab_fr = vocab[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        english_sentence = self.english[index]\n",
        "        french_sentence = self.french[index]\n",
        "        numericalized_en = [self.vocab_en.stoi['<SOS>']]\n",
        "        numericalized_en += self.vocab_en.numericalize(english_sentence)\n",
        "        numericalized_en.append(self.vocab_en.stoi['<EOS>'])\n",
        "        numericalized_en = torch.tensor(numericalized_en)\n",
        "\n",
        "        numericalized_fr = [self.vocab_fr.stoi['<SOS>']]\n",
        "        numericalized_fr += self.vocab_fr.numericalize(french_sentence)\n",
        "        numericalized_fr.append(self.vocab_fr.stoi['<EOS>'])\n",
        "        numericalized_fr = torch.tensor(numericalized_fr)\n",
        "\n",
        "        return numericalized_fr, numericalized_en\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx_fr, pad_idx_en):\n",
        "        self.pad_idx_fr = pad_idx_fr\n",
        "        self.pad_idx_en = pad_idx_en\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        fr = [item[0] for item in batch]\n",
        "        en = [item[1] for item in batch]\n",
        "        fr = pad_sequence(fr, padding_value=self.pad_idx_fr)\n",
        "        en = pad_sequence(en, padding_value=self.pad_idx_en)\n",
        "        return fr, en\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size, shuffle, vocab=None):\n",
        "    dataset = CustomDataset(root_dir, vocab=vocab)\n",
        "    pad_idx_en = dataset.vocab_en.stoi['<PAD>']\n",
        "    pad_idx_fr = dataset.vocab_fr.stoi['<PAD>']\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size, shuffle=shuffle,\n",
        "        collate_fn=MyCollate(pad_idx_fr, pad_idx_en),\n",
        "    )\n",
        "    return dataset, loader"
      ],
      "metadata": {
        "id": "ZwN47vEG_j6g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os.path.join(os.getcwd(), \"data\")):\n",
        "  !wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/019/853/original/data.zip\n",
        "\n",
        "  import zipfile\n",
        "  with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
        "      zip_ref.extractall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxXIQ2Lh_9_m",
        "outputId": "a91f620f-9e66-465d-c09d-a2b8e014067b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-19 04:33:38--  https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/019/853/original/data.zip\n",
            "Resolving d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)... 13.33.28.35, 13.33.28.145, 13.33.28.138, ...\n",
            "Connecting to d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)|13.33.28.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1269302 (1.2M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   1.21M  3.32MB/s    in 0.4s    \n",
            "\n",
            "2022-11-19 04:33:39 (3.32 MB/s) - ‘data.zip’ saved [1269302/1269302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, embeds, max_len, device):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.embed_size = embeds\n",
        "    self.max_len = max_len\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoding = torch.zeros(self.max_len, self.embed_size, device=self.device)\n",
        "    # encoding.requires_grad = False\n",
        "    pos = torch.arange(0, self.max_len, device=self.device)\n",
        "    pos = pos.float().unsqueeze(dim=1)\n",
        "    i = torch.arange(0, self.embed_size, step=2, device=self.device).float()\n",
        "\n",
        "    encoding[:, 0::2] = torch.sin(pos / (10000 ** (i / self.embed_size)))\n",
        "    encoding[:, 1::2] = torch.cos(pos / (10000 ** (i / self.embed_size)))\n",
        "    \n",
        "    batch_size, seq_len = x.size()\n",
        "    return encoding[:seq_len, :].expand(batch_size, seq_len, self.embed_size)"
      ],
      "metadata": {
        "id": "AK6CASdkB7S_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, embed_size, src_vocab_size, trg_vocab_size, src_pad_idx,\n",
        "               num_heads, num_encoder_layers, num_decoder_layers, forward_expansion,\n",
        "               dropout, max_len, device):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.src_word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.src_positional_encoding = PositionalEncoding(embed_size, max_len, device)\n",
        "\n",
        "    self.trg_word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "    self.trg_positional_encoding = PositionalEncoding(embed_size, max_len, device)\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.transformer = nn.Transformer(embed_size, num_heads, num_encoder_layers, num_decoder_layers,\n",
        "                                      forward_expansion, dropout)\n",
        "    \n",
        "    self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "\n",
        "  def make_source_mask(self, x):\n",
        "    # [seq_len, batch]\n",
        "    src_mask = x.permute(1,0) == self.src_pad_idx\n",
        "    # [batch, seq_len]\n",
        "    return src_mask\n",
        "\n",
        "  def forward(self, source, target):\n",
        "    src_seq_len, batch = source.shape\n",
        "    trg_seq_len, batch = target.shape\n",
        "\n",
        "    src_embeddings = self.dropout(self.src_word_embedding(source) + self.src_positional_encoding(source))\n",
        "    trg_embeddings = self.dropout(self.trg_word_embedding(target) + self.trg_positional_encoding(target))\n",
        "\n",
        "    src_padding_mask = self.make_source_mask(source)\n",
        "    trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_len).to(self.device)\n",
        "\n",
        "    out = self.transformer(src_embeddings, trg_embeddings, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask)\n",
        "    out = self.fc_out(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "EQyrSSBUATiu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_set, train_loader = get_loader(\"data/train\", batch_size=batch_size, shuffle=True)\n",
        "val_set, val_loader = get_loader(\"data/val\", batch_size=batch_size, shuffle=True,\n",
        "                                  vocab=[train_set.vocab_en, train_set.vocab_fr])"
      ],
      "metadata": {
        "id": "syKTwvHyFVmz"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "learning_rate = 3e-4\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "src_vocab_size = len(train_set.vocab_fr)\n",
        "trg_vocab_size = len(train_set.vocab_en)\n",
        "embed_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "dropout = 0.10\n",
        "max_len = 1000\n",
        "forward_expansion = 4\n",
        "src_pad_idx = train_set.vocab_en['<PAD>']"
      ],
      "metadata": {
        "id": "KoAawvNoE4z6"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(embed_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers,\n",
        "                    forward_expansion, dropout, max_len, device).to(device)"
      ],
      "metadata": {
        "id": "eLd3sW6iGLv9"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=src_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.1, patience=10, verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "Di6z77eXGAyu"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,e = next(iter(train_loader))\n",
        "f,e = f.to(device), e.to(device)\n",
        "f.shape, e.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxNW833YGHYF",
        "outputId": "b33c9c87-2ffb-4461-ab05-f13086885147"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([38, 128]), torch.Size([34, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op = model(f,e)\n",
        "op.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Ta382WHktb",
        "outputId": "3c8f046d-98f4-4c94-ec51-45db203af915"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([34, 128, 5893])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, sentence, french, english, device, max_length=100):\n",
        "  num_french = [french.stoi['<SOS>']]\n",
        "  num_french += french.numericalize(sentence)\n",
        "  num_french.append(french.stoi['<EOS>'])\n",
        "  num_french = torch.tensor(num_french)\n",
        "  num_french = num_french.unsqueeze(1)\n",
        "  num_french = num_french.to(device)\n",
        "\n",
        "  outputs = [english[\"<SOS>\"]]\n",
        "  for i in range(max_length):\n",
        "    trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(num_french, trg_tensor)\n",
        "\n",
        "    best_guess = output.argmax(2)[-1, :].item()\n",
        "    outputs.append(best_guess)\n",
        "\n",
        "    if best_guess == english.stoi[\"<EOS>\"]:\n",
        "        break\n",
        "  translated_sentence = [english[idx] for idx in outputs]\n",
        "    # remove start token\n",
        "  return \" \".join(translated_sentence[1:])"
      ],
      "metadata": {
        "id": "Oi49v0VRMgH_"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab = train_set.vocab_en\n",
        "french_vocab = train_set.vocab_fr"
      ],
      "metadata": {
        "id": "ozo5_IVrQp_F"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Une petite fille grimpe dans une maisonnette en bois.\"\n",
        "translation = \"A little girl climbing into a wooden playhouse.\"\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    with torch.inference_mode():\n",
        "      print(translate(model, sentence, french_vocab, english_vocab, device))\n",
        "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch_idx, (french, english) in train_loop:\n",
        "        french = french.to(device)\n",
        "        english = english.to(device)\n",
        "        batch_size = english.shape[1]\n",
        "\n",
        "        output = model(french, english[:-1, :])\n",
        "\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        english = english[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, english)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        per_example_loss = loss.data.item()/batch_size\n",
        "        train_loop.set_description(f\"Epoch: {epoch}/{num_epochs}\")\n",
        "        train_loop.set_postfix({\"batch_loss\": loss.data.item(), \"per_example_loss\":per_example_loss})\n",
        "        if epoch % 20 == 0:\n",
        "          torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "          print(\"model saving done\")\n",
        "\n",
        "    mean_loss = sum(losses) / len(losses)\n",
        "    scheduler.step(mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFeq9gwhHpkG",
        "outputId": "93a4bdc1-e6a6-4a7a-e596-2e684dc7a7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obstacle pillar drinks pillar 100 explosion marble marble 100 marble obstacle monster drinks drinks 100 drinks drinks drinks marble grocery retail tape drinks seated watches retail 100 lobby drinks drinks marble obstacle watches drinks watches obstacle union monster tape wrestler 100 lobby obstacle wrestler 100 drinks marble lobby 100 marble monster watches marble drinks drinks tape marble historic pillar 100 obstacle drinks tape watches explosion marble wrestler 100 100 drinks obstacle marble drinks 100 explosion drinks watches drinks marble drinks tape drinks knitting pillar watches 100 drinks lobby marble obstacle watches tape drinks wrestler drinks wrestler watches marble grocery lobby\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a girl in a girl is playing a girl . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl in a pink dress is walking in a little girl in a small pool . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl in a little girl is jumping in a small wooden wooden . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl is climbing a wooden in a wooden area . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl in a wooden hat is walking in a wooden area . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl in a wooden room in a wooden room . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl is climbing a wooden wooden structure into a wooden wooden structure . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a girl climbing a wooden structure in a wooden area . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl in a wooden coat is climbing a wooden wooden structure . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a wooden little girl climbs a wooden wooden wooden wooden in a wooden area . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl is climbing a wooden pole in a <UNK> a hardwood wooden wooden wooden . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl climbing a wooden wooden wooden castle in the woods . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl is climbing a wooden wooden structure in the woods . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl is climbing a wooden playhouse in a wooden room . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a little girl in a wooden room is climbing a wooden fence . <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5gxr5OWXSBHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}