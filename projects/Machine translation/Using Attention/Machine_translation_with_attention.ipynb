{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation using Attention"
      ],
      "metadata": {
        "id": "V2kKQ90hKldp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "g0O5Zm7122fA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "t7jc0Tpp3CS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2012684a-242f-47a5-fedc-4e63618b8fe8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.4.0/fr_core_news_sm-3.4.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 34.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from fr-core-news-sm==3.4.0) (3.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->fr-core-news-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')\n",
        "spacy_fr = spacy.load('fr_core_news_sm')\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, frequency_threshold):\n",
        "        self.itos = {\n",
        "            0: '<PAD>',\n",
        "            1: '<SOS>',\n",
        "            2: '<EOS>',\n",
        "            3: '<UNK>'\n",
        "        }\n",
        "\n",
        "        self.stoi = {\n",
        "            '<PAD>': 0,\n",
        "            '<SOS>': 1,\n",
        "            '<EOS>': 2,\n",
        "            '<UNK>': 3\n",
        "        }\n",
        "\n",
        "        self.frequency_threshold = frequency_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def __getitem__(self, value):\n",
        "      if isinstance(value, int):\n",
        "        return self.itos[value]\n",
        "      else:\n",
        "        return self.stoi[value]\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return []\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "                if frequencies[word] == self.frequency_threshold:\n",
        "                    self.itos[idx] = word\n",
        "                    self.stoi[word] = idx\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi['<UNK>']\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "    def un_numericalize(self, encoding):\n",
        "        return \" \".join([\n",
        "            self.itos[token.data.item()] if token.data.item() in self.itos else self.itos[3]\n",
        "            for token in encoding\n",
        "        ])\n",
        "\n",
        "\n",
        "class EngVocabulary(Vocabulary):\n",
        "    def __init__(self, frequency_threshold):\n",
        "        super().__init__(frequency_threshold)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "class FrVocabulary(Vocabulary):\n",
        "    def __init__(self, frequency_threshold):\n",
        "        super().__init__(frequency_threshold)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer(text):\n",
        "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]"
      ],
      "metadata": {
        "id": "RMMwX_S63HbZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, frequency_threshold_en=2, frequency_threshold_fr=1, vocab=None):\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.english = open(os.path.join(root_dir, \"english.txt\")).read().split(\"\\n\")[:-1]\n",
        "        self.french = open(os.path.join(root_dir, \"french.txt\")).read().split(\"\\n\")[:-1]\n",
        "\n",
        "        if vocab is None:\n",
        "            self.vocab_en = EngVocabulary(frequency_threshold_en)\n",
        "            self.vocab_fr = FrVocabulary(frequency_threshold_fr)\n",
        "            self.vocab_en.build_vocabulary(self.english)\n",
        "            self.vocab_fr.build_vocabulary(self.french)\n",
        "        else:\n",
        "            self.vocab_en = vocab[0]\n",
        "            self.vocab_fr = vocab[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        english_sentence = self.english[index]\n",
        "        french_sentence = self.french[index]\n",
        "        numericalized_en = [self.vocab_en.stoi['<SOS>']]\n",
        "        numericalized_en += self.vocab_en.numericalize(english_sentence)\n",
        "        numericalized_en.append(self.vocab_en.stoi['<EOS>'])\n",
        "        numericalized_en = torch.tensor(numericalized_en)\n",
        "\n",
        "        numericalized_fr = [self.vocab_fr.stoi['<SOS>']]\n",
        "        numericalized_fr += self.vocab_fr.numericalize(french_sentence)\n",
        "        numericalized_fr.append(self.vocab_fr.stoi['<EOS>'])\n",
        "        numericalized_fr = torch.tensor(numericalized_fr)\n",
        "\n",
        "        return numericalized_fr, numericalized_en\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "    def __init__(self, pad_idx_fr, pad_idx_en):\n",
        "        self.pad_idx_fr = pad_idx_fr\n",
        "        self.pad_idx_en = pad_idx_en\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        fr = [item[0] for item in batch]\n",
        "        lengths = torch.tensor([item.shape[0] for item in fr])\n",
        "        en = [item[1] for item in batch]\n",
        "        fr = pad_sequence(fr, padding_value=self.pad_idx_fr)\n",
        "        en = pad_sequence(en, padding_value=self.pad_idx_en)\n",
        "        return (fr, lengths), en\n",
        "\n",
        "\n",
        "def get_loader(root_dir, batch_size, shuffle, vocab=None):\n",
        "    dataset = CustomDataset(root_dir, vocab=vocab)\n",
        "    pad_idx_en = dataset.vocab_en.stoi['<PAD>']\n",
        "    pad_idx_fr = dataset.vocab_fr.stoi['<PAD>']\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size, shuffle=shuffle,\n",
        "        collate_fn=MyCollate(pad_idx_fr, pad_idx_en),\n",
        "    )\n",
        "    return dataset, loader"
      ],
      "metadata": {
        "id": "F3RMY0YlBCHC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/019/853/original/data.zip\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TitP6Vb5BVGZ",
        "outputId": "af214542-97bd-4a47-ac68-3acc48a39d3f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-17 11:23:57--  https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/019/853/original/data.zip\n",
            "Resolving d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)... 18.164.115.154, 18.164.115.106, 18.164.115.84, ...\n",
            "Connecting to d2beiqkhq929f0.cloudfront.net (d2beiqkhq929f0.cloudfront.net)|18.164.115.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1269302 (1.2M) [application/zip]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "\rdata.zip.1            0%[                    ]       0  --.-KB/s               \rdata.zip.1          100%[===================>]   1.21M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-11-17 11:23:57 (29.0 MB/s) - ‘data.zip.1’ saved [1269302/1269302]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, bidirectional=True):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
        "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
        "\n",
        "    def forward(self, source):\n",
        "        # print(\"input shape: \", x.shape, self.embedding)\n",
        "        x, lengths = source\n",
        "        embedding = self.embedding(x)\n",
        "        packed_embeds = pack_padded_sequence(embedding, lengths.to('cpu'), enforce_sorted=False)\n",
        "        # print(\"---------- Encoder ----------\")\n",
        "        # print(\"embedding shape: \", embedding.shape)\n",
        "\n",
        "        encoder_packed_states, (hidden, cell) = self.lstm(packed_embeds)\n",
        "        encoder_states, _ = pad_packed_sequence(encoder_packed_states)\n",
        "\n",
        "        # print(\"output shape: \", encoder_states.shape, hidden.shape, cell.shape)\n",
        "        \n",
        "        # hidden shape: (2, N, hidden_size) if num_layers == 1\n",
        "        # print(hidden[0:1].shape, hidden[1:2].shape)\n",
        "        # print(torch.cat((hidden[0:1], hidden[1:2]), dim=2).shape)\n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "\n",
        "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "\n",
        "        # print(\"output shape: \", encoder_states.shape, hidden.shape, cell.shape)\n",
        "\n",
        "        # encoder_state have hidden state for each time step\n",
        "        # while hidden and cell are only for the rightmost step\n",
        "        return encoder_states, hidden, cell\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers)\n",
        "\n",
        "        self.energy = nn.Linear(hidden_size*3, 1) # small NN to compute attention scores (alpha)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        # print(\"---------- Decoder ----------\")\n",
        "        # since x is only one word so need to add extra dimension of 1\n",
        "        # x.shape: (1, batch_size)\n",
        "        x = x.unsqueeze(0)\n",
        "        embedding = self.embedding(x)\n",
        "        # embedding.shape: (1, batch_size, 300)\n",
        "\n",
        "        sequence_length = encoder_states.shape[0]\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "\n",
        "        # print(\"h_reshaped: \", h_reshaped.shape)\n",
        "\n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
        "        # print(\"energy: \", energy.shape)\n",
        "        attention = self.softmax(energy) # dim=0\n",
        "        # print(\"attention: \", attention.shape)\n",
        "        # shape: seq_length, N, 1\n",
        "        attention = attention.permute(1,2,0)\n",
        "        # shape: N, 1, seq_length\n",
        "        encoder_states = encoder_states.permute(1,0,2)\n",
        "        # shape: N, seq_length, hidden_size*2\n",
        "        \n",
        "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
        "        # (N, 1, hidden_size*2) --> (1, N, hidden_size*2)\n",
        "\n",
        "        lstm_input = torch.cat((context_vector, embedding), dim=2)\n",
        "        # print(\"lstm_input: \", lstm_input.shape)\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "        predictions = self.fc(output)\n",
        "        predictions = predictions.squeeze(0)\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder,  decoder, eng_vocab_size, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.eng_vocab_size = eng_vocab_size\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source[0].shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        # picking 0s because in vocab 0 stands for <PAD> to pad the remaining length\n",
        "        outputs = torch.zeros(\n",
        "            (target_len, batch_size, self.eng_vocab_size)).to(self.device)\n",
        "\n",
        "        encoder_states, hidden, cell = self.encoder(source)\n",
        "\n",
        "        # passing the first character for each sencetence in batch\n",
        "        x = target[0] # <SOS> shape: (1, B, embed_size)\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            best_guess = output.argmax(1)  # returns index of maximum value\n",
        "            x = target[t] if random.random(\n",
        "            ) < teacher_force_ratio else best_guess\n",
        "        return outputs\n",
        "\n",
        "    def predict(self, source, max_len=100):\n",
        "        result = []\n",
        "        result.append(1)  # index of <SOS> token\n",
        "        hidden, cell = self.encoder(source)\n",
        "        hidden, cell = hidden.unsqueeze(1), cell.unsqueeze(1)\n",
        "        x = torch.tensor([1]).to(self.device)\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            best_guess = output.argmax()\n",
        "            result.append(best_guess)\n",
        "            print(best_guess.data.item())\n",
        "            if best_guess.data.item() == 2:\n",
        "                return torch.tensor(result).to(self.device)\n",
        "        result.append(2)  # index of <EOS> token\n",
        "\n",
        "        return torch.tensor(result).to(self.device)"
      ],
      "metadata": {
        "id": "ctIQwMfABUlF"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, train_loader = get_loader(\"data/train\", batch_size=128, shuffle=True)\n",
        "val_set, val_loader = get_loader(\"data/val\", batch_size=128, shuffle=True,\n",
        "                                  vocab=[train_set.vocab_en, train_set.vocab_fr])"
      ],
      "metadata": {
        "id": "FePoknMzL893"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input_size_encoder = len(train_set.vocab_fr)\n",
        "input_size_decoder = len(train_set.vocab_en)\n",
        "output_size = len(train_set.vocab_en)\n",
        "encoder_embedding_size = 300\n",
        "decoder_embedding_size = 300\n",
        "hidden_size = 1024\n",
        "num_layers = 1\n",
        "encoder_dropout = 0.5\n",
        "decoder_dropout = 0.5\n",
        "pad_idx = train_set.vocab_en.stoi[\"<PAD>\"]"
      ],
      "metadata": {
        "id": "X7wlMJrdMJaB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, encoder_dropout, True).to(device)\n",
        "decoder_net = Decoder(input_size_decoder, decoder_embedding_size,\n",
        "                      hidden_size, output_size, num_layers, decoder_dropout).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net, len(train_set.vocab_en), device).to(device)"
      ],
      "metadata": {
        "id": "EpVoFACmL4ZW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "iBPcCWe-SxnB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(f,l),e = next(iter(train_loader))\n",
        "f,e = f.to(device), e.to(device)\n",
        "f.shape, e.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Xz27vEW1vw",
        "outputId": "09cd0fb2-22e1-439a-fd32-5d518a1f3d4c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([35, 128]), torch.Size([28, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op = model((f,l),e)\n",
        "op.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwqC_GyYWnoF",
        "outputId": "7a5c9c91-657b-48a2-f23b-3c035cbd04b1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 128, 5893])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_val_loss = 0\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for batch_idx, ((french, lengths), english) in train_loop:\n",
        "        french = french.to(device)\n",
        "        english = english.to(device)\n",
        "        output = model((french, lengths), english)\n",
        "        output = output[1:].reshape(-1, output.shape[2])\n",
        "        english = english[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, english)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.data.item()\n",
        "        train_loop.set_description(f\"Epoch: {epoch}/{num_epochs}\")\n",
        "        train_loop.set_postfix({\"batch_loss\": loss.data.item(), \"train_loss\":train_loss, \"val_loss\": total_val_loss})\n",
        "        if epoch % 100 == 0:\n",
        "          torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "          print(\"model saving done\")\n",
        "        # ---- validation-----\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        val_loss = 0\n",
        "        val_loop = tqdm(val_loader, total=len(val_loader), leave=False)\n",
        "        for (french, lengths), english in val_loop:\n",
        "            french = french.to(device)\n",
        "            english = english.to(device)\n",
        "            output = model((french, lengths), english)\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            english = english[1:].reshape(-1)\n",
        "            loss = criterion(output, english).data.item()\n",
        "            val_loss += loss\n",
        "\n",
        "            val_loop.set_description(\"Validating\")\n",
        "            val_loop.set_postfix({'val loss': loss})\n",
        "        total_val_loss = val_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5n4zKKdSpR-",
        "outputId": "5e8b9d4d-bb7e-410b-ad28-4b2abb796bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1/1000:  29%|██▊       | 65/227 [00:24<01:03,  2.57it/s, batch_loss=8.68, train_loss=564, val_loss=0]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D6LJO-S3UWaX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}