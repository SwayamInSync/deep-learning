{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVawncRRw78y"
      },
      "source": [
        "# Normalization and pre-tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAtOBX9w781"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lqlwshsw782"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLisAbUXw783",
        "outputId": "bfd681e4-1614-4809-a99c-6b816c505d5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<class 'tokenizers.Tokenizer'>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(type(tokenizer.backend_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTLcmrInw784",
        "outputId": "de71706f-7468-44ee-b715-69cb42702a42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5HfreOiw785",
        "outputId": "69571d0a-82aa-4db0-ab50-fb33fa8e2970"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njlxnAVaw785",
        "outputId": "c62d6b26-496d-4069-c3af-da0fa7573fb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),\n",
              " ('?', (19, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZdIhMGxw786",
        "outputId": "49403fa8-fe74-494b-ca71-026cc653dd89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('▁Hello,', (0, 6)), ('▁how', (7, 10)), ('▁are', (11, 14)), ('▁you?', (16, 20))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Normalization and pre-tokenization",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}