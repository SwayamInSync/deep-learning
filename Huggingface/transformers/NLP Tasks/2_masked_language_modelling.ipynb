{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66985530"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "text = \"This is a great [MASK].\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8]), torch.Size([1, 8, 30522]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.1452e-01, 2.7459e-01, 3.7539e-01,  ..., 3.3268e-01,\n",
       "          5.3011e-01, 6.1277e-01],\n",
       "         [5.6898e-04, 5.0407e-04, 5.8367e-04,  ..., 8.1463e-04,\n",
       "          1.5591e-03, 2.1732e-03],\n",
       "         [5.3732e-04, 3.8677e-04, 5.4660e-04,  ..., 2.0755e-03,\n",
       "          1.1978e-02, 3.7115e-03],\n",
       "         ...,\n",
       "         [6.7616e-01, 7.1718e-01, 6.1386e-01,  ..., 6.4859e-01,\n",
       "          4.3356e-01, 2.3645e-01],\n",
       "         [1.0458e-03, 9.6402e-04, 1.1481e-03,  ..., 4.6911e-03,\n",
       "          5.7508e-03, 2.5998e-02],\n",
       "         [6.1588e-03, 5.6910e-03, 7.5507e-03,  ..., 8.1323e-03,\n",
       "          1.3459e-02, 1.1197e-01]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3066, 3112, 6172, 2801, 8658]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_tokens = torch.topk(mask_token_logits, k=5, dim=1).indices[0].tolist()\n",
    "top_5_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/Users/swayam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b555f4d409f740eba94fe6cfe0eacb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6359293c9e54db296867721b29a4522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (720 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc69642fc2bd43d191b912f99325201e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdd67d68f9d4744ba67d3b72b318489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "def tokenize_and_chunk(batch):\n",
    "    encodings = tokenizer(batch['text'])\n",
    "    encodings['word_ids'] = [encodings.word_ids(idx) for idx in range(len(encodings['input_ids']))]\n",
    "\n",
    "    concatenated = {\n",
    "        k : sum(encodings[k], []) for k in encodings.keys()\n",
    "        # tokenized input already contains [SEP] token after each individual example\n",
    "    }\n",
    "\n",
    "\n",
    "    result = {\n",
    "        k : [t[i : i+chunk_size] for i in range(0, len(concatenated['input_ids']), chunk_size)]\n",
    "        for k,t in concatenated.items()\n",
    "    }\n",
    "\n",
    "    result['labels'] = result['input_ids'].copy() # will be use to evaluate mask predictions\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_dataset = imdb_dataset.map(tokenize_and_chunk, batched=True, remove_columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59929\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 123007\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i rented i am curious - yellow from [MASK] video store because of all the controversy that surrounded [MASK] when it was first released in 1967. i also heard that at first it was seized by u. s. customs if it ever tried to enter this country, therefore being a fan [MASK] films considered \" â‚ˆ \" [MASK] [MASK] had to see [MASK] for [MASK]. < br / > < br / > the [MASK] is centered around a young swedish drama student named lena who [MASK] [MASK] learn everything she can about life. in particular she wants to focus [MASK] attention [MASK] to making some sort of documentary on what the average sw [MASK] [MASK] about certain political issues such\n",
      "as the [MASK] war and race issues in the united states. in net [MASK] politicians and ordinary denizens of stockholm about [MASK] opinions on [MASK], [MASK] has sex with her drama teacher, classmates, and married [MASK]. < br / > < br / > what kills [MASK] [MASK] i am curious - yellow is that 40 years [MASK], this [MASK] considered pornographic [MASK] really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex [MASK] nudity [MASK] a major staple in swedish cinema. even ingmar bergman,\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_dataset[\"train\"][i] for i in range(2)]\n",
    "\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)['input_ids']:\n",
    "    print(tokenizer.decode(chunk))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need `whole word masking` to mask entire word not just one token, HF already had a `DataCollatorForWholeWordMask` but we are creating a new one from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2 # whole word masking probability\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "    # randomly mask word\n",
    "    mask = np.random.binomial(1, wwm_probability, (len(mapping),)) # similar to [1, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
    "    input_ids = feature[\"input_ids\"]\n",
    "    labels = feature[\"labels\"]\n",
    "    new_labels = [-100] * len(labels) # label only those indices where [MASK] is occuring\n",
    "    for word_id in np.where(mask)[0]: # return index where mask is 1\n",
    "        word_id = word_id.item()\n",
    "        for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx] # give them right label\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'word_ids', 'labels'])\n",
      "dict_keys(['input_ids', 'attention_mask', 'word_ids', 'labels'])\n",
      "\n",
      "'>>> [CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u. s. customs if it ever tried to enter this country, therefore being a fan of films considered \" controversial \" i really had to see this for myself. < br / > < br / > the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such'\n",
      "\n",
      "'>>> ['[CLS]', 'i', 'rented', 'i', 'am', 'curious', '-', 'yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', '.', 'i', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', '\"', 'controversial', '\"', 'i', 'really', 'had', 'to', 'see', 'this', 'for', 'myself', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', 'in', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attention', '##s', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'sw', '##ede', 'thought', 'about', 'certain', 'political', 'issues', 'such']'\n",
      "\n",
      "'>>> as [MASK] vietnam war and race issues [MASK] the united states [MASK] [MASK] between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama [MASK] [MASK] [MASK], and married men. < br [MASK] > < br / > what [MASK] me [MASK] [MASK] am [MASK] - yellow is that 40 years [MASK], this was considered [MASK]. [MASK], the sex [MASK] nudity scenes are few and far [MASK], even then it's not [MASK] like some cheaply made porno. while my countrymen mind find it shocking, in [MASK] sex and nudity are a major staple [MASK] [MASK] cinema [MASK] [MASK] ingmar [MASK],'\n",
      "\n",
      "'>>> ['as', '[MASK]', 'vietnam', 'war', 'and', 'race', 'issues', '[MASK]', 'the', 'united', 'states', '[MASK]', '[MASK]', 'between', 'asking', 'politicians', 'and', 'ordinary', 'den', '##ize', '##ns', 'of', 'stockholm', 'about', 'their', 'opinions', 'on', 'politics', ',', 'she', 'has', 'sex', 'with', 'her', 'drama', '[MASK]', '[MASK]', '[MASK]', ',', 'and', 'married', 'men', '.', '<', 'br', '[MASK]', '>', '<', 'br', '/', '>', 'what', '[MASK]', 'me', '[MASK]', '[MASK]', 'am', '[MASK]', '-', 'yellow', 'is', 'that', '40', 'years', '[MASK]', ',', 'this', 'was', 'considered', '[MASK]', '.', '[MASK]', ',', 'the', 'sex', '[MASK]', 'nu', '##dity', 'scenes', 'are', 'few', 'and', 'far', '[MASK]', ',', 'even', 'then', 'it', \"'\", 's', 'not', '[MASK]', 'like', 'some', 'cheap', '##ly', 'made', 'porn', '##o', '.', 'while', 'my', 'country', '##men', 'mind', 'find', 'it', 'shocking', ',', 'in', '[MASK]', 'sex', 'and', 'nu', '##dity', 'are', 'a', 'major', 'staple', '[MASK]', '[MASK]', 'cinema', '[MASK]', '[MASK]', 'ing', '##mar', '[MASK]', ',']'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_dataset[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForWholeWordMask\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForWholeWordMask(tokenizer, mlm_probability=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> [CLS] i rented [MASK] am curious - yellow from myct store because of all the controversy [MASK] surrounded it when it [MASK] first released in 1967. i also heard [MASK] at whisky it was seized [MASK] u. s. customs if it ever tried to [MASK] this country, therefore being a fan of [MASK] considered \" controversial [MASK] [MASK] [MASK] had to [MASK] this evaluate myself. [MASK] br [MASK] > < br / > [MASK] plot [MASK] centered around a [MASK] swedish drama student named lena who wants to learn everything she [MASK] [MASK] [MASK] [MASK] in [MASK] she wants to focus her attentions to making some sort of documentary on what the average swede [MASK] about certain political issues such'\n",
      "\n",
      "'>>> ['[CLS]', 'i', 'rented', '[MASK]', 'am', 'curious', '-', 'yellow', 'from', 'my', '##ct', 'store', 'because', 'of', 'all', 'the', 'controversy', '[MASK]', 'surrounded', 'it', 'when', 'it', '[MASK]', 'first', 'released', 'in', '1967', '.', 'i', 'also', 'heard', '[MASK]', 'at', 'whisky', 'it', 'was', 'seized', '[MASK]', 'u', '.', 's', '.', 'customs', 'if', 'it', 'ever', 'tried', 'to', '[MASK]', 'this', 'country', ',', 'therefore', 'being', 'a', 'fan', 'of', '[MASK]', 'considered', '\"', 'controversial', '[MASK]', '[MASK]', '[MASK]', 'had', 'to', '[MASK]', 'this', 'evaluate', 'myself', '.', '[MASK]', 'br', '[MASK]', '>', '<', 'br', '/', '>', '[MASK]', 'plot', '[MASK]', 'centered', 'around', 'a', '[MASK]', 'swedish', 'drama', 'student', 'named', 'lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', '[MASK]', '[MASK]', '[MASK]', '[MASK]', 'in', '[MASK]', 'she', 'wants', 'to', 'focus', 'her', 'attention', '##s', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'sw', '##ede', '[MASK]', 'about', 'certain', 'political', 'issues', 'such']'\n",
      "\n",
      "'>>> as the vietnam war and race issues in the [MASK] states. in between asking politicians and ordinary scars [MASK] [MASK] of stockholm [MASK] their opinions [MASK] politics, she has [MASK] with her drama teacher, classmates, and married [MASK] [MASK] < br / [MASK] < br / > what [MASK] me [MASK] i am curious - yellow is that 40 sophisticated ago, this was [MASK] pornographic. really, [MASK] sex and nudity scenes are few and [MASK] between, even then it [MASK] [MASK] not à¸£ like some [MASK] [unused481] made porno. while my countrymen [MASK] find it shocking, in reality sex and nudity are a major staple in swedish [MASK] [MASK] [MASK] ingmar bergman [MASK]'\n",
      "\n",
      "'>>> ['as', 'the', 'vietnam', 'war', 'and', 'race', 'issues', 'in', 'the', '[MASK]', 'states', '.', 'in', 'between', 'asking', 'politicians', 'and', 'ordinary', 'scars', '[MASK]', '[MASK]', 'of', 'stockholm', '[MASK]', 'their', 'opinions', '[MASK]', 'politics', ',', 'she', 'has', '[MASK]', 'with', 'her', 'drama', 'teacher', ',', 'classmates', ',', 'and', 'married', '[MASK]', '[MASK]', '<', 'br', '/', '[MASK]', '<', 'br', '/', '>', 'what', '[MASK]', 'me', '[MASK]', 'i', 'am', 'curious', '-', 'yellow', 'is', 'that', '40', 'sophisticated', 'ago', ',', 'this', 'was', '[MASK]', 'pornographic', '.', 'really', ',', '[MASK]', 'sex', 'and', 'nu', '##dity', 'scenes', 'are', 'few', 'and', '[MASK]', 'between', ',', 'even', 'then', 'it', '[MASK]', '[MASK]', 'not', 'à¸£', 'like', 'some', '[MASK]', '[unused481]', 'made', 'porn', '##o', '.', 'while', 'my', 'country', '##men', '[MASK]', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', 'and', 'nu', '##dity', 'are', 'a', 'major', 'staple', 'in', 'swedish', '[MASK]', '[MASK]', '[MASK]', 'ing', '##mar', 'bergman', '[MASK]']'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swayam/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/data/data_collator.py:948: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_dataset[\"train\"][i] for i in range(2)]\n",
    "batch = data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/swayam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-e25b6f8f7c19cd58.arrow and /Users/swayam/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-19539aea64e9d239.arrow\n"
     ]
    }
   ],
   "source": [
    "train_size = 10000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_dataset[\"train\"].train_test_split(test_size, train_size, shuffle=True, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swayam/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/data/data_collator.py:948: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666184c3a9034659852694019c7e2538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40.60905919901439"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "math.exp(eval_results['eval_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acclererate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1996, 5016, 21843, 1998, 4165, 2010, 14315, 1011, 2007, 2010, 2300, 13077, 8840, 2378, 23095, 8134, 4634, 2125, 2010, 6700, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 1005, 1055, 1037, 9467, 2008, 1996, 2058, 1011, 9474, 8292, 29577, 2015, 27604, 2091, 1996, 4639, 3267, 1997, 1996, 24566, 3152, 2044, 2023, 4443, 1012, 2348, 1996, 17889, 12274, 10820, 24566, 3152, 2052, 2145, 4013, 17668, 1999, 1996, 2086, 2000, 2272, 1010, 2027, 2052, 6524, 3921, 1996, 3348, 5574, 1997, 2023, 3185, 1012, 102, 101, 2066, 24566, 1996, 23957, 2158, 1006, 4673, 1007, 1010, 2069, 2062, 2061, 1012, 2045, 1005, 1055, 2062, 1997, 2673, 1010, 2062, 4176, 1010, 2062, 9426, 3060, 6946, 1010, 1998, 5019, 1999, 2029, 1996, 2245, 2442, 2022, 1010, 2065, 2023, 2001, 2204, 2007, 2093], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 282, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 307, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 323, 323, 324, 325, 326, 327, 328, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, None, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42], 'labels': [1996, 5016, 21843, 1998, 4165, 2010, 14315, 1011, 2007, 2010, 2300, 13077, 8840, 2378, 23095, 8134, 4634, 2125, 2010, 6700, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 1005, 1055, 1037, 9467, 2008, 1996, 2058, 1011, 9474, 8292, 29577, 2015, 27604, 2091, 1996, 4639, 3267, 1997, 1996, 24566, 3152, 2044, 2023, 4443, 1012, 2348, 1996, 17889, 12274, 10820, 24566, 3152, 2052, 2145, 4013, 17668, 1999, 1996, 2086, 2000, 2272, 1010, 2027, 2052, 6524, 3921, 1996, 3348, 5574, 1997, 2023, 3185, 1012, 102, 101, 2066, 24566, 1996, 23957, 2158, 1006, 4673, 1007, 1010, 2069, 2062, 2061, 1012, 2045, 1005, 1055, 2062, 1997, 2673, 1010, 2062, 4176, 1010, 2062, 9426, 3060, 6946, 1010, 1998, 5019, 1999, 2029, 1996, 2245, 2442, 2022, 1010, 2065, 2023, 2001, 2204, 2007, 2093]}\n"
     ]
    }
   ],
   "source": [
    "# explanation of below next code cell\n",
    "batch = downsampled_dataset['train'][:3]\n",
    "for t in zip(*batch.values()): # ([input_ids], [attention_mask], [word_ids], [labels])\n",
    "    s = dict(zip(batch, t)) # {'input_ids':[input_ids], 'attention_mask': [attention_mask], ....}\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we saw that `DataCollatorForLanguageModeling` also applies random masking with each evaluation, so weâ€™ll see some fluctuations in our perplexity scores with each training run. One way to eliminate this source of randomness is to apply the masking once on the whole test set, and then use the default data collator in ðŸ¤— Transformers to collect the batches during evaluation. To see how this works, letâ€™s implement a simple function that applies the masking on a batch, similar to our first encounter with `DataCollatorForLanguageModeling`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # Create a new \"masked\" column for each column in the dataset\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee69a59e310649af8822e96a2d0b2f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up the dataloaders as usual, but weâ€™ll use the `default_data_collator` from ðŸ¤— Transformers for the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator # this is imp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and run\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(model_name, save_function=accelerator.save)\n",
    "if accelerator.is_main_process:\n",
    "    tokenizer.save_pretrained(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ee8db5012278e1cd548cc347586dafd6ca78c2be11f6247083f5051ead8f988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
